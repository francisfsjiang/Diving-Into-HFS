% !TEX root = ../report.tex
%
\chapter{构建, 运行与运行时分析}
\label{ch:env}

\section{构建}

构建{\Hadoop}非常简单, 首先需要安装\emph{Apache Ant}, Ant是一个将JAVA软件编译, 测试,
部署等步骤集成起来的自动化工具, 在安装好Ant之后,
在common, hdfs, mapreduce目录下分别执行``ant jar''命令,
ant系统会自动地根据build.xml和build target, 自动下载所需的依赖.

\InsertFigure{env/cr0.png}{编译成功}

\InsertFigure{env/cr0.1.png}{}

同理, 在hdfs, mapreduce中执行相同的命令即可

\section{配置}

本节介绍{\Hadoop}文件系统的基本配置, 包括使用HDFS和使用本地文件系统的例子.
由于环境条件的限制, 加之本文的重点在于对{\Hadoop}文件系统架构的分析,
在此, 针对{\HDFS}, 我们只在本机上配置一个单namenode, 单datanode
的伪集群环境, 但是相关操作都仿照集群操作进行.

为了本节配置说明的普遍性, 没有使用上面手动编译的版本, 而是使用了已经编译好的版本来进行配置,
手动编译的版本配置与此类似.

\section{SSH}

首先, 为了能够自动地启动所有的node, 我们需要在集群机器间配置SSH
服务, 这样就可以通过脚本来远程启动node上指定的服务.

首先要在集群的各台电脑上安装ssh服务, 然后

\InsertFigure{env/cr1.png}{生成密匙对}

生成密匙对.

\InsertFigure{env/cr2.png}{部署密匙}

将密匙对拷贝到集群各节点的用户目录下的.ssh文件夹

\InsertFigure{env/cr3.png}{}

验证是否可以使用ssh命令免密码登录

\InsertFigure{env/cr4.png}{}

成功之后进入下一步

\section{JAVA环境配置}

在这里我们需要设置三个环境变量, 分别是{\JH}, {\CPATH}, {\HHOME}

\InsertFigure{env/cr5.png}{}

{\JH}和{\CPATH}按照基本的Java环境配置即可, {\HHOME}指向的是
我们将{\Hadoop}0.21版本的文件解压到的位置, 里面目录结构如下

\InsertFigure{env/cr6.png}{}

\section{{\Hadoop}配置}

首先是对于本地文件系统, {\Hadoop}不需要进行配置, 使用默认配置,
使用命令即可

\InsertFigure{env/cr7.png}{}

\InsertFigure{env/cr8.png}{}

如果使用HDFS作为文件系统, 那么需要做如下的配置

首先是conf/core-site.xml, 这是设置了{\Hadoop}主要的参数

\InsertFigure{env/cr9.png}{}

其中fs.default.name选项指定了默认的文件系统,
此时指定了默认的文件系统为hdfs, 其namenode的ip地址
为10.211.55.31, 端口为9000.

hadoop.tmp.dir指定了hdfs中datanode存放文件块的位置.

然后是conf/hdfs-site.xml, 这里设置了HDFS的相关参数

\InsertFigure{env/cr10.png}{}

其中dfs.replication指定了文件block副本的数量

dfs.http.address指定了namenode的http服务端口, 可以使用浏览器访问这个端口, 来
观察整个系统的运行情况, 效果如下

\InsertFigure{env/cr11.png}{}

dfs.datanode.http.address指定了datanode的http服务端口, 可以使用浏览器访问这个端口, 来
观察整对应的datanode的运行情况, 效果如下

\InsertFigure{env/cr12.png}{}

dfs.datanode.address, 指定了datanode服务的监听端口

dfs.datanode.ipc.address, 指定了datanode的ipc服务监听端口

接下来是master何slave节点的配置, 分别对应namenode和datanode

conf/master

\InsertFigure{env/cr13.png}{}

conf/slaves

\InsertFigure{env/cr14.png}{}

接下来执行bin/start-all.sh来启动namenode和datanode, 这一步同样会在namenode上启动job tracker来
准备执行MapReduce任务

\InsertFigure{env/cr15.png}{}

然后可以执行如下的命令来观察启动成功后的效果

\section{运行过程分析}

现在我们来对{\Hadoop}文件系统的运行过程进行分析

从{\Hadoop}命令的执行脚本开始, bin/hadoop

\InsertFigure{env/cr16.png}{}

可以看到, 如果执行命令的第二个参数是``dfs'', 那么则会执行如下的语句, 可以这里会调用另外一个脚本hdfs/bin/hdfs

\InsertFigure{env/cr17.png}{}

hdfs脚本如下所示

\InsertFigure{env/cr18.png}{}

可以看到hdfs脚本最后会调用java命令, 并添加相关的虚拟机选项, 指定{\CPATH}, 如果仔细观察, 在bin/hadoop这个脚本中, 在最后也有类似的命令, 如果第二个参数指定为``fs'', 那么则会执行bin/hadoop尾部的命令, 而在此处然后最后执行main所在的目标类是org.apache.hadoop.fs.FsShell

\InsertFigure{env/cr19.png}{}

在加上调试参数之后, 执行如下命令

	 ./hadoop dfs -ls

运行结果如下

\InsertFigure{env/cr20.png}{}

根据上面的观察, 可以发现, {\Hadoop}文件系统的运行基于Shell脚本,
Shell脚本可以根据执行的命令的不同, 设置合适的{\CPATH}等相关参数.

了解了{\Hadoop}, 运行脚本相关机制, 接下来进入IDE的配置, 这样便于使用调试器来观察.

我们没有使用Eclipse, 而是使用JetBrains的IntelliJ IDEA作为开发环境,
主要是因为Eclipse的稳定性欠佳, 而且IDEA是一款在各方面都十分优秀的JAVA IDE,
所以, 我们此次的相关实验内容都在IDEA下完成, 使用的IDEA版本是14.1.4,
操作系统使用的是MAC OSX 10.10.3.

首先, 新建一个空的工程, 名为\emph{HadoopCommon}

\InsertFigure{env/cr21.png}{}

\InsertFigure{env/cr22.png}{}

在Project Structure中的Project页面配置使用的JDK为JDK1.6

\InsertFigure{env/cr23.png}{}

在Modules页面加入一个新的Module, 使用默认的JAVA EE7 即可, Module Name为java

\InsertFigure{env/cr24.png}{}

\InsertFigure{env/cr25.png}{}

然后将Hadoop-release-0.21.0的common中的代码以及core-default.xml和log4j.properties复制到工程目录java/src下, 如下图所示

\InsertFigure{env/cr26.png}{}

然后将{\Hadoop}依赖的jar包复制到lib/中, 这些依赖可以通过解析Ant的Build.xml获得,
也可以通过提取Ant编译好的工程中的Build目录中的相关jar包来完成.

然后回到Project Structure配置页面, 打开Modules -> Dependencies选项卡, 可以看到如下界面

\InsertFigure{env/cr27.png}{}

点击添加JARs or Dictionarys

\InsertFigure{env/cr28.png}{}

选择刚才添加的java/lib/目录

\InsertFigure{env/cr29.png}{}

然后配置运行参数, 点击Run -> Edit Configuration, 添加一个Application

\InsertFigure{env/cr30.png}{}

添加好之后在里面输入相关的参数, 主要是配置Main Class为org.apache.hadoop.fs.FsShell,
在Program Arguments里输入要执行的命令行参数, 如图所示

\InsertFigure{env/cr31.png}{}

好了, 现在完成了基本的配置, 可以运行程序了, 点击Run, 可以看到如下的运行结果

\InsertFigure{env/cr32.png}{}

那么现在可以在程序中添加断点, 来观察程序的运行过程,
函数的入口在org.apache.hadoop.fs.FsShell类中的main函数, 添加断点之后,
点击Run - > Debug来开始调试, 可以看到程序开始位置的断点如下

\InsertFigure{env/cr33.png}{}

根据对源码的阅读, 可以发现, FsShell类实现了Tool接口,
其运行过程主要交给另一个类--ToolRunner负责, 这个工具类的主要作用是,
解析程序传入的参数, 加载程序设置, 然后调用传入的Tool类的run方法, 具体执行的方法如下所示

\InsertFigure{env/cr34.png}{}

现在进入FsShell的run方法, 这个方法根据输入的命令行参数,
如``-ls'', ``-put''等, 调用不同的函数来处理

\InsertFigure{env/cr35.png}{}

当我们的输入参数为``-ls''时, run方法会执行到此处

\InsertFigure{env/cr36.png}{}

可以看到我们没有提供第二个路径参数,
这里默认就设置了Path为当前目录``Path.CUR\_{}ID'', 即``.''.

接下来进入ls的具体执行步骤

\InsertFigure{env/cr37.png}{}

首先, 程序会用传入的path字符串构造一个Path对象, 然后使用Path对象的getFileSystem方法,
这个方法会接着调用FileSystem的静态方法get, 首先在CACHE对象中寻找是否有已创建的对应文件系统,
如果有就直接返回被cache的对象

\InsertFigure{env/cr38.png}{}

如果CACHE中没有, 则会最终调用FileSystem的createFileSystem, 根据默认的配置文件,
以及Path的uri中的Scheme来判断将要生成的具体的FileSystem对象

\InsertFigure{env/cr39.png}{}

然后回到ls函数执行过程, 可以看到这里调用了生成的FileSystem对象的globStatus方法,
此方法会返回匹配输入文件名的所有文件或文件夹的FileStatus对象, 然后根据每个对象是文件或者文件夹,
将结果打印到终端.

注意这里的srcFS是一个org.apache.hadoop.fs.LocalFileSystem对象

\InsertFigure{env/cr40.png}{}

接下来改变一下参数, 现在将core-default.xml中的fs.defaultFS的值,
改为我们刚才设置的HDFS的namenode所在的位置

\InsertFigure{env/cr41.png}{}

现在重新进入调试

\InsertFigure{env/cr42.png}{}

可以看到现在生成的srcFS是一个org.apache.hadoop.hdfs.DistributedFileSystem对象,

以上内容说明, {\Hadoop}文件系统的Shell入口在使用过程中, 会通过读取配置文件,
以及检测输入的uri的scheme, 来定位所需要的继承了FileSystem类的具体类,
然后通过调用这些具体类的方法来执行具体的功能.

\section{实例分析: copyFromLocal}
\label{sec:env:cfl}

为了清楚地说明使用Hadoop文件shell命令行执行命令的具体过程,
以求展示出Hadoop文件系统的具体执行流程, 来反映其架构.

Hadoop Shell命令运行的入口位于org.apache.hadoop.fs.FsShell的main函数中,
FsShell类实现了Tool接口.

\InsertFigure{env/cfl0.png}{}

程序运行开始后, 会首先生成一个FsShell对象, 然后将此对象传入ToolRunner类的静态方法run,
ToolRunner类是一个工具类, 功能是配置并运行实现了Tool接口的对象, 在此方法中,
会生成一个根据配置文件生成的Configuration对象, 此对象保存了系统的所有设置,
生成好后会将此conf设置为FsShell对象的配置.

\InsertFigure{env/cfl1.png}{}

然后进入FsShell的run方法, 在此方法中, 传入的命令行参数将被解析, 根据不同的命令,
调用具体的方法执行, 例如`-copyFromLocal`参数会对应到copyFromLocal方法.

\InsertFigure{env/cfl2.png}{}

\InsertFigure{env/cfl3.png}{}

之后进入copyFromLocal函数,
该函数会根据传入参数指定的Path路径的Scheme来获取对应的具体FileSystem对象,
然后对该FileSystem对象调用copyFromLocal方法, 来执行具体的操作.

\InsertFigure{env/cfl4.png}{}

从这里可以发现, Hadoop FileSystem主要的架构就是两层, 一层是用户接口,
比如FsShell、FileContext, 二层是具体的FileSystem, 比如DistributedFileSystem.
用户接口并不关心下层的具体文件系统的实现方式, 只关心如何去调用FileSystem的方法.
只有具体的FileSystem类才会去关心具体的文件系统的实现方法.
